{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6c547d2",
   "metadata": {},
   "source": [
    "# Home assignment 5\n",
    "\n",
    "You should prepare solutions of the presented problems in this Jupyter Notebook and submit it in the Google Classroom.\n",
    "\n",
    "Please, rename the Jupyter Notebook that you will submit as ```Surname_assignment5.ipynb```, where instead of ```Surname``` you write your family name. A solution of every problem should be placed below of the corresponding problem statement.\n",
    "\n",
    "After the running commands (Kernel -> Restart & Run All) all cells in your file have to run correctly. Please check this before submission.\n",
    "\n",
    "## Problem 1 (10 pts)\n",
    "\n",
    "- (2 pts) Prove that gradients, computed in two sequential points, generated by the steepest gradient descent, are orthogonal\n",
    "\n",
    "- (5 pts) Prove that if the objective function is $f(x) = \\frac{1}{2}x^{\\top}Qx - b^{\\top}x$, $Q \\in \\mathbb{S}^n_{++}$, step size is chosen according to the steepest descent rule and $x^0 - x^*$ is parallel to the eigenvector of the matrix $Q$, then gradient descent converges after one iteration.\n",
    "\n",
    "- Show that for the function $f(x) = x^{\\top}x$ the steepest gradient descent converges after one iteration using\n",
    "    - (1 pts) previous claim\n",
    "    - (2 pts) direct computations\n",
    "\n",
    "```python\n",
    "# Your solution is here\n",
    "```\n",
    "\n",
    "## Problem 2 (12 pts)\n",
    "\n",
    "Consider the signal denoising problem and smooth approximation of the total-variation regularizer.\n",
    "Let $x_{cor}$ be a corrupted signal that you should recover.\n",
    "To perform such recovering you can use total variation regulaizer $TV(x) = \\sum_{i=0}^{n-1} |x_{i+1} - x_i|$.\n",
    "However, it is non-smooth and therefore direct usage of the first order method is prohibitive.\n",
    "To address this issue one can replace the original total variationa regularizer with its smooth approximation\n",
    "\n",
    "$$ ATV(x) = \\sum_{i=0}^{n-1} \\left(\\sqrt{\\varepsilon^2 + (x_{i+1} - x_i)^2} - \\varepsilon\\right),$$\n",
    "\n",
    "where $\\varepsilon > 0$ is a hyper-parameter controling the accuracy of approximation.\n",
    "\n",
    "Thus, the final denoising problem is the following\n",
    "\n",
    "$$ \\min_x \\|x - x_{cor}\\|_2^2 + \\lambda ATV(x), $$\n",
    "\n",
    "where $\\lambda > 0$ is a hyperparameter that controls the smoothness of the resulting signal $x^*$.\n",
    "Use the corrupted signal and hyperparameters from the cell below. Choose appropriate value for $\\lambda$, it is typically of the order 10-100.\n",
    "\n",
    "- (5 pts) Compare convergence of gradient descent, heavy-ball method and fast gradient method to solve this problem with a number of constant step sizes $\\alpha = \\{1, 10^{-1}, 10^{-2}, 10^{-3}, 10^{-4} \\}$. Make a conclusion about the methods performances and their dependence on step size value.\n",
    "\n",
    "- (5 pts) Check numerically how the value of $\\varepsilon$ affects the resulting signal quality and the convergence of the aforementioned methods. Make the necessary plots that confirm your conclusions. Explain the observed behaviour.\n",
    "\n",
    "- (2 pts) Visualize the resulting signals from all three considered optimizers (choose the best result for every optimizer) on the one frame with the original corrupted signal. Do you observe some differences in the solutions?\n",
    "\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "n = 5000\n",
    "EPSILON = 0.001\n",
    "xcor = np.hstack([np.ones(1000) + 0.1 * np.random.randn(1000), 5 * np.ones(1000) + 0.1 *np.random.randn(1000),\n",
    "                   -3*np.ones(1000) + 0.1 *np.random.randn(1000), -2 * np.ones(1000) + 0.1 *np.random.randn(1000),\n",
    "                  3 * np.ones(1000) + 0.1 *np.random.randn(1000)])\n",
    "plt.plot(xcor)\n",
    "```\n",
    "\n",
    "```python\n",
    "# Your solution is here\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ec030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd90d4f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02007ab5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d37dcc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2cec30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04129dab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
