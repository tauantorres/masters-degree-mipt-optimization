{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bef4d82",
   "metadata": {},
   "source": [
    "# `Problem 1`:\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "`1.1.` Check convexity of the following sets:\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "- `(1 pts)` $$ \\mathbb{X} = \\{ x \\in \\mathbb{R}^n \\mid x_i \\geq 0, \\; \\| x\\|_{\\infty} \\leq 1 \\} $$\n",
    "\n",
    "---\n",
    "\n",
    "* `Definition` **Convex Set**:\n",
    "\n",
    "A set $S \\subseteq \\mathbb{R}^n$ is called ***convex*** if:\n",
    "1. $\\forall x, y \\in S$;\n",
    "2. $\\forall \\theta \\in [0, 1]$\n",
    "\n",
    "Then the point defined as:\n",
    "$$\n",
    "\\theta x + (1 - \\theta) y \\in S\n",
    "$$.\n",
    "\n",
    "* **Meaning**: For any two points in the set, the entire line segment connecting them lies within the set.\n",
    "\n",
    "---\n",
    "\n",
    "### `Solution:`\n",
    "\n",
    "* **Step 1**:\n",
    "\n",
    "Let's recall that:\n",
    "$$\n",
    "\\|x\\|_\\infty \\le 1 \\iff \\max_i |x_i| \\le 1 \\iff |x_i|\\le 1 \\;\\; \\forall i\n",
    "$$\n",
    "\n",
    "Therefore, the set can be rewritten as:\n",
    "$$\n",
    "\\mathbb{X}=\\{x\\in\\mathbb{R}^n \\mid x_i\\ge 0,\\; |x_i|\\le 1 \\;\\forall i\\}\n",
    "$$\n",
    "\n",
    "But since we already have $x_i\\ge 0$, the condition $|x_i|\\le 1$ is equivalent to $x_i\\le 1$. Thus, we have:\n",
    "$$\n",
    "\\mathbb{X}=\\{x\\in\\mathbb{R}^n \\mid 0\\le x_i\\le 1 \\;\\forall i\\}\n",
    "$$\n",
    "\n",
    "* **Step 2**:\n",
    "\n",
    "Let's take 2 arbitrary points $x, y \\in \\mathbb{X}$. Then for each coordinate $i$, we have:\n",
    "$$\n",
    "0 \\le x_i \\le 1 \\quad \\text{and} \\quad 0 \\le y_i \\le 1\n",
    "$$\n",
    "\n",
    "Next, we define their convex combination:\n",
    "$$\n",
    "z := \\theta x + (1 - \\theta) y, \\quad \\theta \\in [0, 1]\n",
    "$$\n",
    "\n",
    "So component-wise, we have:\n",
    "$$\n",
    "z_i = \\theta x_i + (1 - \\theta) y_i, \\quad \\forall i, \\quad \\theta \\in [0, 1]\n",
    "$$\n",
    "\n",
    "* **Step 3**:\n",
    "\n",
    "Now, checking their lower bound $z_i \\ge 0$. Since we have:\n",
    "1. $\\theta \\ge 0$\n",
    "2. $(1 - \\theta) \\ge 0$\n",
    "3. $x_i$ and $y_i$ are **non-negative**.\n",
    "\n",
    "Therefore, we can write:\n",
    "$$\n",
    "z_i = \\theta x_i + (1 - \\theta) y_i \\ge 0 \\implies z_i \\ge 0 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "* **Step 4**:\n",
    "\n",
    "Now, checking their upper bound $z_i \\le 1$. Since we have:\n",
    "1. $\\theta \\le 1$\n",
    "2. $(1 - \\theta) \\le 1$\n",
    "3. $x_i \\le 1$ and $y_i \\le 1$.\n",
    "\n",
    "Therefore, we can write:\n",
    "$$\n",
    "z_i = \\theta x_i + (1 - \\theta) y_i \\le \\theta \\cdot 1 + (1 - \\theta) \\cdot 1 = \\theta + 1 - \\theta = 1 \\\\\n",
    "\\Rightarrow z_i \\le 1 \\quad \\forall i\n",
    "$$\n",
    "\n",
    "* **Step 5**:\n",
    "\n",
    "We ahve proven that $\\forall i$:\n",
    "$$\n",
    "0 \\le z_i \\le 1\n",
    "$$\n",
    "\n",
    "which means that $z \\in \\mathbb{X}$.\n",
    "\n",
    "Threfore, $\\mathbb{X}$ satisfies the definition of convexity.\n",
    "\n",
    "$\\mathbb{X} \\text{ is a convex set}$.\n",
    "\n",
    "---\n",
    "\n",
    "- `(1 pts)` $$ \\mathbb{G} = \\{ X \\in \\mathbb{R}^{n \\times n} \\mid y^\\top X y \\leq 0 \\}, $$\n",
    "where $y$ is a given fixed vector of the proper dimension\n",
    "\n",
    "---\n",
    "\n",
    "### `Solution:`\n",
    "\n",
    "* **Step 1**:\n",
    "\n",
    "let's identify what elements we have:\n",
    "\n",
    "1. The variable $X$ is a matrix in $\\mathbb{R}^{n \\times n}$.\n",
    "2. The expression $y^\\top X y$ is a quadratic form (a scalar) defined by the matrix $X$ and the vector $y$.\n",
    "3. The $y$ is a fixed vector in $\\mathbb{R}^n$.\n",
    "\n",
    "Now, we need to understand the full expression as a funtion of $X$:\n",
    "$$\n",
    "g(X) := y^\\top X y\n",
    "$$\n",
    "\n",
    "* **Step 2**:\n",
    "\n",
    "Let's again use the definition of convexity:\n",
    "\n",
    "* Let $X_1, X_2 \\in \\mathbb{G}$ be two arbitrary matrices in the set. Then, by definition of the set, we have:\n",
    "$$\n",
    "y^\\top X_1 y \\le 0 \\quad \\text{and} \\quad y^\\top X_2 y \\le 0\n",
    "$$\n",
    "\n",
    "* Now, we need to check if the convex combination of these two matrices also belongs to the set $\\mathbb{G}$. Let's define:\n",
    "$$\n",
    "X_\\theta := \\theta X_1 + (1 - \\theta) X_2, \\quad \\theta \\in [0, 1]\n",
    "$$\n",
    "\n",
    "* Let's verify if $X_\\theta \\in \\mathbb{G}$, by evaluating the quadratic form:\n",
    "\n",
    "$$\n",
    "y^\\top \\cdot X_\\theta \\cdot y  = y^\\top \\cdot (\\theta X_1 + (1 - \\theta) X_2) \\cdot y \\\\\n",
    "y^\\top \\cdot (\\theta X_1 + (1 - \\theta) X_2) \\cdot y = y^\\top \\cdot \\theta X_1 \\cdot y + y^\\top \\cdot (1 - \\theta) X_2 \\cdot y \\\\\n",
    "y^\\top \\cdot (\\theta X_1 + (1 - \\theta) X_2) \\cdot y = \\theta (y^\\top X_1 y) + (1 - \\theta)(y^\\top X_2 y)\n",
    "$$\n",
    "\n",
    "* **Step 3**:\n",
    "\n",
    "Since we have that:\n",
    "1. $y^\\top \\cdot X_1 \\cdot y \\le 0$ and $y^\\top \\cdot X_2 \\cdot y \\le 0$;\n",
    "2. $\\theta$ and $(1 - \\theta)$ are non-negative for $\\theta \\in [0, 1]$;\n",
    "\n",
    "Therefore, we can conclude that:\n",
    "\n",
    "1. $\\theta \\cdot (y^\\top X_1 y) \\le 0$\n",
    "2. $(1 - \\theta)(y^\\top X_2 y) \\le 0$\n",
    "\n",
    "Hence, summing these two inequalities, we get:\n",
    "$$\n",
    "\\Rightarrow y^\\top \\cdot X_\\theta \\cdot y = \\theta (y^\\top X_1 y) + (1 - \\theta)(y^\\top X_2 y) \\le 0 \\\\\n",
    "\\Rightarrow X_\\theta \\in \\mathbb{G}, \\quad X_\\theta \\text{ is convex}\n",
    "$$\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "`1.2.` Check convexity/concavity of the following functions, whose domain is $\\mathbb{R}^n$ or $\\mathbb{S}^n$\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "- `(1 pts)` $$f(x) = \\max \\{ c^\\top x + b, \\quad \\|x\\|_2 \\}$$\n",
    "\n",
    "---\n",
    "\n",
    "* `Definition` **Convex Function**:\n",
    "\n",
    "A function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is called ***convex*** if $\\text{dom } f$ is a convex set and:\n",
    "1. $\\forall x, y \\in \\text{dom } f$;\n",
    "2. $\\forall \\theta \\in [0, 1]$\n",
    "Then the following inequality holds:\n",
    "$$\n",
    "f(\\theta x + (1 - \\theta) y) \\le \\theta f(x) + (1 - \\theta) f(y)\n",
    "$$\n",
    "\n",
    "* **Meaning**: The line segment (chord) between any two points on the graph of the function lies above or on the graph.\n",
    "\n",
    "---\n",
    "\n",
    "* `Property` **Pointwise Maximum and Convexity**:\n",
    "\n",
    "If $f_1, \\ldots, f_m$ are **convex functions**, then their **pointwise maximum** $f(x) = \\max \\{ f_1(x), \\ldots, f_m(x) \\}$ is also a **convex function**.\n",
    "\n",
    "* **Meaning**: If each individual function \"curves upward,\" the outer boundary of their maximum will also \"curve upward.\"\n",
    "\n",
    "---\n",
    "\n",
    "### `Solution:`\n",
    "\n",
    "* **Step 1**:\n",
    "\n",
    "Lets analyze each component of the function separately:\n",
    "\n",
    "1. We have the function $f_1(x) = c^\\top x + b$, which is an **affine function**. **Affine functions** are both **convex** and **concave** because they satisfy the definition of convexity with equality:\n",
    "> $$\n",
    "> f_1(\\theta x + (1 - \\theta) y) = \\theta f_1(x) + (1 - \\theta) f_1(y), \\quad \\forall x, y \\in \\mathbb{R}^n, \\; \\theta \\in [0, 1]\n",
    "> $$\n",
    "\n",
    "2. We have the function $f_2(x) = \\|x\\|_2$, which is the **Euclidean norm** ($L_2$ norm). The **Euclidean norm** is a well-known **convex function**. Every norm satisfies the triangle inequality and positive homogeneity, which are key properties that ensure convexity:\n",
    "> 1. $\\|x + y\\|_2 \\le \\|x\\|_2 + \\|y\\|_2$ (Triangle Inequality)\n",
    "> 2. $\\|\\alpha x\\|_2 = |\\alpha| \\|x\\|_2$ for any scalar $\\alpha$ (Positive Homogeneity)\n",
    "\n",
    "> Using these 3 properties, we can show that for any $x, y \\in \\mathbb{R}^n$ and $\\theta \\in [0, 1]$:\n",
    "> $$\n",
    "> \\| \\theta x + (1 - \\theta) y \\|_2 \\le  \\|\\theta x\\|_2 + \\|(1 - \\theta) y\\|_2 = \\theta \\|x\\|_2 + (1 - \\theta) \\|y\\|_2\n",
    "> $$\n",
    "\n",
    "\n",
    "* **In conclusion**:\n",
    "Both $f_1(x)$ and $f_2(x)$ are **convex functions**.\n",
    "\n",
    "\n",
    "* **Step 2**:\n",
    "\n",
    "Let's apply the **Property of Pointwise Maximum**:\n",
    "\n",
    "Since both $f_1(x) = c^\\top x + b$ and $f_2(x) = \\|x\\|_2$ are **convex functions**, their **pointwise maximum**:\n",
    "$$\n",
    "f(x) = \\max \\{ c^\\top x + b, \\quad \\|x\\|_2 \\}\n",
    "$$\n",
    "is also a **convex function**.\n",
    "\n",
    "---\n",
    "\n",
    "- `(1 pts)` $$f(x) = \\|Ax - b\\|_{50} + \\exp\\left(-\\sum_{i=1}^n x_i\\right)$$\n",
    "\n",
    "---\n",
    "\n",
    "* `Property` **Composition with Affine Mapping**:\n",
    "\n",
    "Let's suppose $f: \\mathbb{R}^k \\to \\mathbb{R}$ and $A \\in \\mathbb{R}^{k \\times n}$, $b \\in \\mathbb{R}^k$.\n",
    "The function $g: \\mathbb{R}^n \\to \\mathbb{R}$ defined by:\n",
    "\n",
    "$$\n",
    "g(x) = f(Ax + b)\n",
    "$$\n",
    "\n",
    "is **convex** if $f$ is **convex**, and **concave** if $f$ is **concave**.\n",
    "\n",
    "---\n",
    "\n",
    "* `Property` **Non-negative Weighted Sum**:\n",
    "\n",
    "If $f_1, \\ldots, f_m$ are **convex functions** and $\\alpha_1, \\ldots, \\alpha_m \\ge 0$ are non-negative scalars, then the weighted sum defined as:\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^{m} \\alpha_i f_i(x)\n",
    "$$\n",
    "is also a **convex function**.\n",
    "\n",
    "---\n",
    "\n",
    "### `Solution:`\n",
    "\n",
    "* **Step 1**:\n",
    "\n",
    "Let's analyze each component of the function separately:\n",
    "\n",
    "1. `Term 01`- $L_{50}$ norm:\n",
    "$$\n",
    "h_1(x) = \\|Ax - b\\|_{50}\n",
    "$$\n",
    "\n",
    " - The $L_p$ norm $\\| . \\|_p$ is convex for $p \\ge 1$. Here $p = 50 \\ge 1$.\n",
    " - The function $h_1(x)$ is a composition of the convex function $\\| . \\|_{50}$ with the affine mapping $Ax - b$.\n",
    " - The composition of affine mapping conserves the convexity. Thus, $h_1(x)$ is **convex**.\n",
    "\n",
    "\n",
    "2. `Term 02`- Exponential term:\n",
    "$$\n",
    "h_2(x) = \\exp\\left(-\\sum_{i=1}^n x_i\\right)\n",
    "$$\n",
    "\n",
    " - Let $u(x) = -\\sum_{i=1}^n x_i$, which is an affine function of x (hence both convex and concave).\n",
    " - The exponential function $e^{u}$ is convex $u$.\n",
    " - Again, the composition of affine mapping conserves the convexity. Thus, $h_2(x)$ is **convex**.\n",
    "\n",
    "* **Step 2**:\n",
    "\n",
    "Since both $h_1(x)$ and $h_2(x)$ are **convex functions**, the sum of convex functions is also a **convex function** (by the **Property of Non-negative Weighted Sum**):\n",
    "$$\n",
    "f(x) = h_1(x) + h_2(x) = \\|Ax - b\\|_{50} + \\exp\\left(-\\sum_{i=1}^n x_i\\right)\n",
    "$$\n",
    "is a **convex function**.\n",
    "\n",
    "---\n",
    "\n",
    "- `(2 pts)` $f(X) = \\lambda_{\\min}(X)$ is the smallest eigenvalue of a given symmetric square matrix $X$, and $X \\in \\mathbb{S}^n$\n",
    "\n",
    "---\n",
    "\n",
    "* `Definition` **Concave Function**:\n",
    "\n",
    "A function $f: \\mathbb{R}^n \\to \\mathbb{R}$ is called ***concave*** if $\\text{dom } f$ is a convex set and:\n",
    "1. $\\forall x, y \\in \\text{dom } f$;\n",
    "2. $\\forall \\theta \\in [0, 1]$\n",
    "Then the following inequality holds:\n",
    "$$\n",
    "f(\\theta x + (1 - \\theta) y) \\ge \\theta f(x) + (1 - \\theta) f(y)\n",
    "$$\n",
    "\n",
    "* **Meaning**: The line segment (chord) between any two points on the graph of the function lies below or on the graph.\n",
    "\n",
    "---\n",
    "\n",
    "* `Lemma` **Variational Characterization of Eigenvalues**:\n",
    "\n",
    "For any symmetric matrix $X \\in \\mathbb{S}^n$, the smallest eigenvalue $\\lambda_{\\min}(X)$ can be expressed as the solution to the following optimization problem:\n",
    "$$\n",
    "\\lambda_{\\min}(X) = \\inf \\{y^\\top \\cdot X \\cdot y \\text{ | } \\|y\\|_2 = 1  \\}\n",
    "$$\n",
    "\n",
    "* **meaning**: This means that $\\lambda_{\\min}(X)$ is the **infimum** of a collection of functions $g_y(X) = y^\\top \\cdot X \\cdot y$, each of which is a function of $X$ for a fixed $y$, indexed by all vectors $y$ on the unit sphere $\\{y \\in \\mathbb{R}^n \\mid \\|y\\|_2 = 1\\}$.\n",
    "\n",
    "---\n",
    "\n",
    "* `Property` **Pointwise Infimum and Convexity**:\n",
    "\n",
    "If $f(x, y)$ is linear (or convave) in $x$ for every $y \\in S$, then the function $g(x)$ defined as the pointwise infimum over $y$:\n",
    "$$\n",
    "g(x) = \\inf_{y \\in S} f(x, y)\n",
    "$$\n",
    "is a **concave function**.\n",
    "\n",
    "---\n",
    "\n",
    "### `Solution:`\n",
    "\n",
    "* **Step 1**:\n",
    "\n",
    "For the symmetric matrix $X$, the smallest eigenvalue can be expressed as:\n",
    "$$\n",
    "\\lambda_{\\min}(X) = \\inf \\{y^\\top \\cdot X \\cdot y \\text{ | } \\|y\\|_2 = 1  \\}\n",
    "$$\n",
    "\n",
    "\n",
    "* **Step 2**:\n",
    "\n",
    "For a fixed vector $y$ with $\\|y\\|_2 = 1$, we define the function $g_y(X) \\colon \\mathbb{S}^n \\to \\mathbb{R}$ as:\n",
    "$$\n",
    "g_y(X) = y^\\top \\cdot X \\cdot y = \\sum_{i=1}^{n} \\sum_{j=1}^{n} y_i \\cdot y_j \\cdot X_{ij} \n",
    "$$\n",
    "\n",
    "Since $g_y(X)$ is a weighted sum of the elements of $X$, it is a **linear function** of $X$ for each fixed $y$.\n",
    "\n",
    "* **Step 3**:\n",
    "\n",
    "Applying the **Property of Pointwise Infimum**, since $g_y(X)$ is **linear** in $X$ for each fixed $y$ with $\\|y\\|_2 = 1$, we can conclude that the function defined as the pointwise infimum over all such $y$:\n",
    "$$\n",
    "\\lambda_{\\min}(X) = \\inf \\{y^\\top \\cdot X \\cdot y \\text{ | } \\|y\\|_2 = 1  \\}\n",
    "$$\n",
    "is a **concave function**.\n",
    "\n",
    "* **Step 4**:\n",
    "\n",
    "Since  $\\lambda_{\\min}(X)$ is the **Pointwise Infimum** of a family of **linear functions** $g_y(X) = y^\\top \\cdot X \\cdot y$:\n",
    "\n",
    "1. Each $g_y(X)$ is **concave**;\n",
    "2. The infimum of a concave functions is also **concave**;\n",
    "\n",
    "Therefore, we conclude that $f(X) = \\lambda_{\\min}(X)$ satisfies the definition of concavity.\n",
    "\n",
    "$$\n",
    "\\lambda_{\\min}(\\theta X_1 + (1 - \\theta) X_2) \\ge \\theta \\lambda_{\\min}(X_1) + (1 - \\theta) \\lambda_{\\min}(X_2), \\quad \\forall X_1, X_2 \\in \\mathbb{S}^n, \\; \\theta \\in [0, 1]\n",
    "$$\n",
    "\n",
    "Hence, $f(X) = \\lambda_{\\min}(X) \\text{ is a concave function}$.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9e66e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9efb4978",
   "metadata": {},
   "source": [
    "`1.3.` Solve analytically the following optimization problems\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "- `(1 pts)`\n",
    "$$\n",
    "\\max_{x \\in \\mathbb{R}^n} \\left[y^\\top x - \\frac12 \\|x\\|_2^2 \\right]\n",
    "$$\n",
    "\n",
    "Where $y$ is the given fixed vector\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "\n",
    "### `Solution:`\n",
    "\n",
    "* **Step 1**: \n",
    "\n",
    "Define the objective function:\n",
    "$$\n",
    "f(x) = y^\\top x - \\frac{1}{2} \\|x\\|_2^2 = y^\\top x - \\frac{1}{2} x^\\top x\n",
    "$$\n",
    "\n",
    "This is a strictly concave quadratic function.\n",
    "\n",
    "* **Step 2**: \n",
    "\n",
    "Let's compute the gradient of $f(x)$ with respect to $x$:\n",
    "$$\n",
    "\\nabla_x f(x) = \\nabla_x (y^\\top x) - \\nabla_x \\left( \\frac{1}{2} x^\\top x \\right) = y - x\n",
    "$$\n",
    "\n",
    "* **Step 3**: \n",
    "Optimality condition, setting the gradient to zero:\n",
    "$$\n",
    "y - x = 0 \\implies x^* = y\n",
    "$$\n",
    "\n",
    "* **Step 4**:\n",
    "Hessian of $f(x)$:\n",
    "$$\n",
    "\\nabla_x^2 f(x) = \\nabla_x (y - x) = - I\n",
    "$$\n",
    "\n",
    "The Hessian is negative definite, confirming that $f(x)$ is strictly concave.\n",
    "\n",
    "* **Step 5**:\n",
    "Thus, the maximum is attained at:\n",
    "$$\n",
    "x^* = y\n",
    "$$\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "- `(2 pts)`\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^n}  \\frac12 \\|x - y \\|_2^2 + \\lambda \\sum_{i=1}^{n-1} (x_i - x_{i+1})^2 \n",
    "$$\n",
    "\n",
    "Where $y$ is a given fixed vector of the proper dimension\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### `Solution:`\n",
    "\n",
    "* **Step 1**:\n",
    "\n",
    "Let's define the **Difference Operator** matrix $D \\in \\mathbb{R}^{(n-1) \\times n}$, such that:\n",
    "$$\n",
    "D = \\begin{bmatrix}\n",
    "1 & -1 & 0 & \\cdots & 0 \\\\\n",
    "0 & 1 & -1 & \\cdots & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "0 & 0 & 0 & \\cdots & -1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "which in another way can be written as:\n",
    "$$\n",
    "(Dx)_i = x_i - x_{i+1}, \\quad i = 1, 2, \\ldots, n-1\n",
    "$$\n",
    "\n",
    "\n",
    "Then, we can rewrite the second term of the objective function as:\n",
    "$$\n",
    "\\sum_{i=1}^{n-1} (x_i - x_{i+1})^2 = \\|Dx\\|_2^2 = (Dx)^\\top (Dx) = x^\\top D^\\top D x\n",
    "$$\n",
    "\n",
    "Thus, we can define the objective function as:\n",
    "$$\n",
    "f(x) = \\frac{1}{2} \\|x - y\\|_2^2 + \\lambda x^\\top D^\\top D x\n",
    "$$\n",
    "\n",
    "And the optimization problem can be rewritten as:\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^n} \\frac{1}{2} \\|x - y\\|_2^2 + \\lambda x^\\top D^\\top D x\n",
    "$$\n",
    "\n",
    "\n",
    "* **Step 2**:\n",
    "\n",
    "Let's compute the gradient of $f(x)$ with respect to $x$:\n",
    "$$\n",
    "\\nabla_x f(x) = \\nabla_x \\left( \\frac{1}{2} \\|x - y\\|_2^2 \\right) + \\nabla_x \\left( \\lambda x^\\top D^\\top D x \\right)\n",
    "$$\n",
    "\n",
    "Calculating each term separately:\n",
    "1. For the first term:\n",
    "$$\n",
    "\\nabla_x \\left( \\frac{1}{2} \\|x - y\\|_2^2 \\right) = x - y\n",
    "$$\n",
    "2. For the second term:\n",
    "$$\n",
    "\\nabla_x \\left( \\lambda x^\\top D^\\top D x \\right) = 2 \\lambda D^\\top D x\n",
    "$$\n",
    "\n",
    "Combining these results, we have:\n",
    "$$\n",
    "\\nabla_x f(x) = (x - y) + 2 \\lambda D^\\top D x = x - y + 2 \\lambda D^\\top D x\n",
    "$$\n",
    "\n",
    "* **Step 3**:\n",
    "\n",
    "Optimality condition, setting the gradient to zero:\n",
    "$$\n",
    "x - y + 2 \\lambda D^\\top D x = 0\n",
    "$$\n",
    "\n",
    "Rearranging the terms, we get:\n",
    "$$\n",
    "(I + 2 \\lambda D^\\top D) x = y\n",
    "$$\n",
    "\n",
    "> **Note**: The matrix $(I + 2 \\lambda D^\\top D)$ is positive definite, hence invertible. Why?\n",
    ">    - $I$ is positive definite.\n",
    ">    - $D^\\top D$ is positive semi-definite (since for any vector $z$, $z^\\top D^\\top D z = \\|Dz\\|_2^2 \\ge 0$).\n",
    ">    - The sum of a positive definite matrix and a positive semi-definite matrix is positive definite.\n",
    "\n",
    "\n",
    "\n",
    "* **Step 4**:\n",
    "\n",
    "Thus, the optimal solution $x^*$ can be expressed as:\n",
    "$$\n",
    "x^* = (I + 2 \\lambda D^\\top D)^{-1} y\n",
    "$$\n",
    "\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "- `(2 pts)`\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^n} c^\\top x \\quad \\text{s.t. } x \\geq 0\n",
    "$$\n",
    "\n",
    "Where $c$ is a given fixed vector of the proper dimension\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### `Solution:`\n",
    "\n",
    "* **Step 1**:\n",
    "\n",
    "Define the objective function:\n",
    "$$\n",
    "f(x) = c^\\top x = \\sum_{i=1}^{n} c_i x_i\n",
    "$$\n",
    "\n",
    "Each component is independent. Therefore, we need to analyze the objective function component-wise.\n",
    "In other words, we can rewrite the optimization problem as:\n",
    "$$\n",
    "\\min_{x_i \\ge 0} \\sum_{i=1}^{n} c_i x_i \\quad i = 1, 2, \\ldots, n\n",
    "$$\n",
    "\n",
    "\n",
    "* **Step 2**:\n",
    "\n",
    "Now, we analyze each component $c_i x_i$ based on the sign of $c_i$:\n",
    "\n",
    "1. If $c_i > 0$:\n",
    "   - The function $c_i x_i$ is increasing in $x_i$.\n",
    "   - To minimize it under the constraint $x_i \\ge 0$, we set $x_i^* = 0$.\n",
    "\n",
    "2. If $c_i < 0$:\n",
    "   - The function $c_i x_i$ is decreasing in $x_i$.\n",
    "   - The objective function is unbounded below as $x_i$ increases.\n",
    "   - Therefore, there is no finite optimal solution in this case. (for $x_i \\to +\\infty$ the objective $c_i x_i \\to -\\infty$).\n",
    "\n",
    "3. If $c_i = 0$:\n",
    "    - The function $c_i x_i = 0$ for all $x_i \\ge 0$.\n",
    "    - Any $x_i \\ge 0$ is optimal.\n",
    "\n",
    "\n",
    "* **Step 3**:\n",
    "\n",
    "Thus, the optimal solution $x^*$ can be summarized as:\n",
    "$$x_i^* = \\begin{cases}\n",
    "0, & \\text{if } c_i > 0 \\\\\n",
    "\\text{unbounded below}, & \\text{if } c_i < 0 \\\\\n",
    "\\text{any } x_i \\ge 0, & \\text{if } c_i = 0\n",
    "\\end{cases} \\quad i = 1, 2, \\ldots, n\n",
    "$$\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "- `(1 pts)`\n",
    "$$\n",
    "\\min_{x \\in \\mathbb{R}^n} \\frac12 \\|x - y \\|_2 \\quad \\text{s.t. } \\|x\\|_2 \\leq 1\n",
    "$$\n",
    "\n",
    "Where $y$ is a given fixed vector of the proper dimension\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "### `Solution:`\n",
    "\n",
    "* **Geometric Interpretation**:\n",
    "\n",
    "This problem can be interpreted geometrically as finding the point $x$ within or on the boundary of the unit ball (defined by $\\|x\\|_2 \\le 1$) that is closest to the point $y$ in terms of Euclidean distance.\n",
    "\n",
    "\n",
    "* **Step 1**: ($y$ inside the unit ball):\n",
    "\n",
    "If $\\|y\\|_2 \\le 1$, then the point $y$ itself lies within the feasible region defined by the constraint $\\|x\\|_2 \\le 1$.\n",
    "Thus, the optimal solution is: $x^* = y$\n",
    "\n",
    "* **Step 2**: ($y$ outside the unit ball):\n",
    "\n",
    "If $\\|y\\|_2 > 1$, then the point $y$ lies outside the feasible region.\n",
    "In this case, the closest point on the boundary of the unit ball to $y$ can be found by projecting $y$ onto the unit ball.\n",
    "The projection of $y$ onto the unit ball is given by:\n",
    "$$\n",
    "x^* = \\frac{y}{\\|y\\|_2}\n",
    "$$\n",
    "\n",
    "* **Step 3**: (Final solution):\n",
    "\n",
    "Thus, the optimal solution $x^*$ can be summarized as:\n",
    "$$\n",
    "x^* = \\begin{cases}\n",
    "y, & \\text{if } \\|y\\|_2 \\le 1 \\\\\n",
    "\\frac{y}{\\|y\\|_2}, & \\text{if } \\|y\\|_2 > 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Which can be compactly written as:\n",
    "$$\n",
    "x^* = \\frac{y}{\\max(1, \\|y\\|_2)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2b839e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c88edf6d",
   "metadata": {},
   "source": [
    "# `Problem 2`\n",
    "\n",
    "`2.1. (2 pts)` Verify with cvxpy that your solutons of problems from 1.3 are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b8ef6a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Problem      | Status          | Max Difference \n",
      "---------------------------------------------\n",
      "Polishing not needed - no active set detected at optimal point\n",
      "3.1 (Max)    | MISMATCH        | 5.69e-06\n",
      "3.2 (Reg)    | MATCH           | 1.11e-16\n",
      "3.3 (LP)     | MATCH           | 4.64e-08\n",
      "3.4 (Proj)   | MATCH           | 9.63e-09\n"
     ]
    }
   ],
   "source": [
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Tolerance for numerical comparison\n",
    "EPSLON = 1e-6\n",
    "\n",
    "def verify_solutions_with_tolerance(eps: float = EPSLON) -> None:\n",
    "    n = 5\n",
    "    lam = 1.0\n",
    "    y_val = np.array([1.5, -0.5, 2.0, 0.0, -1.0])\n",
    "    c_val = np.array([1.0, 0.5, 2.0, 0.1, 0.8]) \n",
    "\n",
    "    print(f\"{'Problem':<12} | {'Status':<15} | {'Max Difference':<15}\")\n",
    "    print(\"-\" * 45)\n",
    "\n",
    "    # Problem 3.1:\n",
    "    x1 = cp.Variable(n)\n",
    "    obj1 = cp.Maximize(y_val @ x1 - 0.5 * cp.sum_squares(x1))\n",
    "    cp.Problem(obj1).solve()\n",
    "    diff1 = np.linalg.norm(x1.value - y_val, np.inf)\n",
    "    status1 = \"MATCH\" if diff1 < eps else \"MISMATCH\"\n",
    "    print(f\"{'3.1 (Max)':<12} | {status1:<15} | {diff1:.2e}\")\n",
    "\n",
    "    # Problem 3.2:\n",
    "    x2 = cp.Variable(n)\n",
    "    diff_term = cp.sum_squares(x2[:-1] - x2[1:])\n",
    "    obj2 = cp.Minimize(0.5 * cp.sum_squares(x2 - y_val) + lam * diff_term)\n",
    "    cp.Problem(obj2).solve()\n",
    "    \n",
    "    D = np.eye(n) - np.eye(n, k=1)\n",
    "    D = D[:-1, :]\n",
    "    x_analytical_2 = np.linalg.solve(np.eye(n) + 2 * lam * D.T @ D, y_val)\n",
    "    diff2 = np.linalg.norm(x2.value - x_analytical_2, np.inf)\n",
    "    status2 = \"MATCH\" if diff2 < eps else \"MISMATCH\"\n",
    "    print(f\"{'3.2 (Reg)':<12} | {status2:<15} | {diff2:.2e}\")\n",
    "\n",
    "    # Problem 3.3:\n",
    "    x3 = cp.Variable(n)\n",
    "    obj3 = cp.Minimize(c_val @ x3)\n",
    "    cp.Problem(obj3, [x3 >= 0]).solve()\n",
    "    x_analytical_3 = np.zeros(n)\n",
    "    diff3 = np.linalg.norm(x3.value - x_analytical_3, np.inf)\n",
    "    status3 = \"MATCH\" if diff3 < eps else \"MISMATCH\"\n",
    "    print(f\"{'3.3 (LP)':<12} | {status3:<15} | {diff3:.2e}\")\n",
    "\n",
    "    # Problem 3.4:\n",
    "    x4 = cp.Variable(n)\n",
    "    obj4 = cp.Minimize(0.5 * cp.norm(x4 - y_val, 2))\n",
    "    cp.Problem(obj4, [cp.norm(x4, 2) <= 1]).solve()\n",
    "    x_analytical_4 = y_val / max(1, np.linalg.norm(y_val, 2))\n",
    "    diff4 = np.linalg.norm(x4.value - x_analytical_4, np.inf)\n",
    "    status4 = \"MATCH\" if diff4 < eps else \"MISMATCH\"\n",
    "    print(f\"{'3.4 (Proj)':<12} | {status4:<15} | {diff4:.2e}\")\n",
    "\n",
    "\n",
    "verify_solutions_with_tolerance(eps=EPSLON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0652f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de35cf40",
   "metadata": {},
   "source": [
    "`2.2. (1 pts)` Write a paragraph about the most interesting topic from the course and the least interesting topic and motivate your choice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7a521c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "04bb4945",
   "metadata": {},
   "source": [
    "`2.3. (2 pts)` What are the key factors/ingredients that can help in successful exploitation of the stochastic gradient based optimizers?\n",
    "\n",
    "\n",
    "Stochastic gradient–based optimization methods (SGD, Adam, RMSProp, etc.) succeed when several fundamental ingredients are present.\n",
    "Here are the most important factors that I could find:\n",
    "\n",
    "1. Proper learning rate (step size):\n",
    "- Too large → divergence\n",
    "- Too small → extremely slow convergence\n",
    "- Often needs decay schedules or adaptive scaling (Adam, RMSProp)\n",
    "\n",
    "---\n",
    "\n",
    "2. Gradient noise with good statistical properties:\n",
    "\n",
    "Stochastic methods rely on unbiased gradient estimates:\n",
    "- $\\mathbb{E}[g_t] = \\nabla f(x_t)$\n",
    "- Variance should be controlled\n",
    "- Mini-batches help reduce noise\n",
    "\n",
    "If gradients are biased or noise is too large, convergence is compromised.\n",
    "\n",
    "---\n",
    "\n",
    "3. Smoothness of the objective:\n",
    "\n",
    "Better success when:\n",
    "- The objective has Lipschitz-continuous gradients\n",
    "- No extremely sharp curvature regions\n",
    "- No pathological saddle points\n",
    "\n",
    "Smoothness ensures stable updates.\n",
    "\n",
    "---\n",
    "\n",
    "4. Proper initialization:\n",
    "\n",
    "- Good initial points avoid poor local minima\n",
    "- In deep learning, initialization dramatically affects training dynamics\n",
    "\n",
    "---\n",
    "\n",
    "5. Variance reduction techniques (optional but powerful):\n",
    "\n",
    "Such as:\n",
    "- Momentum\n",
    "- AdaM / RMSProp\n",
    "- SVRG / SAGA\n",
    "- Averaging iterates\n",
    "\n",
    "They stabilize training and accelerate convergence.\n",
    "\n",
    "---\n",
    "\n",
    "6. Batch normalization / data preprocessing:\n",
    "\n",
    "Well-conditioned data → well-conditioned gradients → faster convergence.\n",
    "\n",
    "---\n",
    "\n",
    "7. Appropriate model architecture / parameterization:\n",
    "\n",
    "For neural networks (what I believe that we will be learning in the next semesters):\n",
    "- Residual connections\n",
    "- Normalization layers\n",
    "- Smooth activation functions\n",
    "\n",
    "These significantly help SGD-based optimizers.\n",
    "\n",
    "---\n",
    "\n",
    "8. Regularization:\n",
    "\n",
    "Techniques like weight decay, dropout, data augmentation make the optimization landscape easier to navigate.\n",
    "\n",
    "---\n",
    "\n",
    "## `Final Summary`:\n",
    "\n",
    "SGD-based optimizers succeed when:\n",
    "* Learning rates are tuned\n",
    "* Variance of gradient noise is controlled\n",
    "* The function is smooth enough\n",
    "* Initialization is good\n",
    "* Variance reduction techniques are used\n",
    "* Data/model normalization is applied\n",
    "* Regularization stabilizes the landscape\n",
    "\n",
    "These elements together create the conditions under which stochastic gradient methods are stable, efficient, and effective.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c939c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
